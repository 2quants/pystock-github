#!/bin/sh
version="0.5.0"

# Arguments that will be passed from command line
s3_bucket=""
script_dir=`pwd`
working_dir=`pwd`
end_date=`date +%Y%m%d`

# Required executables
aws_bin="aws"
crawler_bin="pystock-crawler"

# Same time zone with US stock market
TZ="America/New_York"

# Maximum compression level
GZIP="-9"

print_version() {
    echo "pystock-dailycrawl v$version"
}

print_help() {
    print_version
    echo "Usage:"
    echo "  dailycrawl [-b S3_BUCKET] [-c SCRIPT_DIR] [-w WORKING_DIR] [-s START_DATE] [-e END_DATE]"
    echo "  dailycrawl (-h | --help)"
    echo "  dailycrawl (-v | --version)"
}

error() {
    echo "Error: $1"
}

# Check if a directory exists
check_dir() {
    if [ ! -d "$1" ]; then
        error "directory '$1' does not exist"
        exit 1
    fi
}

# Check if an executable exists
check_bin() {
    hash $1 2>/dev/null || { error "'$1' cannot be found in PATH: $PATH"; exit 2; }
}

# Add a directory into PATH if it does not exist in PATH
add_path() {
    if [ -d "$1" ] && [[ ":$PATH:" != *":$1:"* ]]; then
        PATH="${PATH:+"$PATH:"}$1"
    fi
}

# Check if a date (YYYYMMDD) is a holiday
is_holiday() {
    # Saturday or Sunday?
    weekday=`date -j -f "%Y%m%d" $1 +%u`
    if [ "$weekday" = 6 -o "$weekday" = 7 ]; then
        echo $1
    else
        # US holiday?
        year=`date -j -f "%Y%m%d" $1 +%Y`
        holiday_file="$script_dir/holidays/$year.txt"
        cat $holiday_file | grep $1
    fi
}

# Get the previous date of a date (YYYYMMDD)
get_prev_date() {
    date -j -v -1d -f "%Y%m%d" $1 +%Y%m%d
}

# Some paths may be only added after login, which makes some commands such as 'aws'
# unavailable in crontab's @reboot. Add them here just in case.
add_path "/usr/local/bin"
add_path "/sbin"

# Parse command arguments
while test "$1" != ""
do
    case $1 in
        --help|-h)
            print_help
            exit 0
            ;;
        --version|-v)
            print_version
            exit 0
            ;;
        --s3-bucket|-b)
            shift
            s3_bucket="$1"
            ;;
        --script-dir|-c)
            shift
            script_dir="$1"
            ;;
        --working-dir|-w)
            shift
            working_dir="$1"
            ;;
        --start-date|-s)
            shift
            start_date="$1"
            ;;
        --end-date|-e)
            shift
            end_date="$1"
            ;;
        *)
            shift
            ;;
    esac
done

if [ ! "$s3_bucket" ]; then
    error "must specify an S3 bucket"
    exit 1
fi

# Check directories
check_dir $working_dir
check_dir $script_dir

# Check if required commands are available
check_bin $aws_bin
check_bin $crawler_bin

# If start_date is not specified, set it to the first previous
# non-holiday (counting backward from end_date)
if [ ! "$start_date" ]; then
    start_date=`get_prev_date $end_date`
    while [ `is_holiday $start_date` ]
    do
        start_date=`get_prev_date $start_date`
    done
fi

echo "S3 bucket: $s3_bucket"
echo "Script directory: $script_dir"
echo "Working directory: $working_dir"
echo "Start date: $start_date"
echo "End date: $end_date"

exit 0

# /usr/local/bin isn't in PATH on reboot
export PATH=$PATH:/usr/local/bin

export TZ="America/New_York"
export GZIP="-9"

S3_BUCKET="my-s3-bucket"
WORKING_DIR="/home/pystock"
CRAWLER_BIN="/usr/local/bin/pystock-crawler"
AWS_BIN="/usr/local/bin/aws"
SHUTDOWN_BIN="/sbin/shutdown"

echo "============="
echo " Daily Crawl "
echo "============="

log "Waiting for network to be ready"
while [ true ]
do
    ping -c 1 google.com > /dev/null
    if [ "$?" = "0" ]; then
        log "Network is ready"
        break;
    else
        log "Network is not ready"
        sleep 1
    fi
done

cd $WORKING_DIR
log "Working directory: `pwd`"

if [ "$1" ]; then
    today=`date +%Y%m%d -d $1`
    today_dir=`date +%Y/%m/%d -d $1`
else
    today=`date +%Y%m%d`
    today_dir=`date +%Y/%m/%d`
fi

if [ "$2" ]; then
    yesterday=`date +%Y%m%d -d $2`
else
    yesterday=`date -d 'yesterday' +%Y%m%d`
fi

log "Today: $today"
log "Yesterday: $yesterday"

out_dir="./$today_dir"
log "Creating output directory: $out_dir"
mkdir -p $out_dir

log "Crawling symbols"
$CRAWLER_BIN symbols NYSE,NASDAQ -o $out_dir/symbols.txt -l $out_dir/symbols.log

log "Crawling prices"
$CRAWLER_BIN prices $out_dir/symbols.txt -o $out_dir/prices.csv -s $yesterday -e $today -l $out_dir/prices.log

log "Crawling reports"
$CRAWLER_BIN reports $out_dir/symbols.txt -o $out_dir/reports.csv -s $today -l $out_dir/reports.log

cd $out_dir

log "Compressing $out_dir/symbols.*"
tar -czf symbols.tar.gz symbols.*

log "Compressing $out_dir/prices.*"
tar -czf prices.tar.gz prices.*

log "Compressing $out_dir/reports.*"
tar -czf reports.tar.gz reports.*

log "Uploading $out_dir/symbols.tar.gz"
$AWS_BIN s3 cp symbols.tar.gz s3://$S3_BUCKET/$today_dir/symbols.tar.gz

log "Uploading $out_dir/prices.tar.gz"
$AWS_BIN s3 cp prices.tar.gz s3://$S3_BUCKET/$today_dir/prices.tar.gz

log "Uploading $out_dir/reports.tar.gz"
$AWS_BIN s3 cp reports.tar.gz s3://$S3_BUCKET/$today_dir/reports.tar.gz

# log "Powering off after 10 mins"
# $SHUTDOWN_BIN -P 10
